{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "* tensorflow\n",
    "* tqdm\n",
    "* numpy\n",
    "* opencv\n",
    "* keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2eae18e91307>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# tqdm is for progress bar\n",
    "import tqdm\n",
    "import load\n",
    "# glob is a very useful tool for finding pathnames recursively\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to_npy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idhi to_npy.py lo unna code\n",
    "\n",
    "ratio = 0.95\n",
    "image_size = 256 # image size ni 128 nundi 256 ki maarcha.. paper lo 256x256 image size undhi\n",
    "\n",
    "x = []\n",
    "paths = glob.glob('./images/*')\n",
    "for path in paths:\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.resize(img, (image_size, image_size))\n",
    "# cv2.COLOR_BGR2RGB is to convert images to RGB, if our images are grayscale use- cv2.COLOR_GRAY2RGB as it may return errors\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    x.append(img)\n",
    "\n",
    "x = np.array(x, dtype=np.uint8)\n",
    "np.random.shuffle(x)\n",
    "\n",
    "p = int(ratio * len(x))\n",
    "x_train = x[:p]\n",
    "x_test = x[p:]\n",
    "\n",
    "if not os.path.exists('./npy'):\n",
    "    os.mkdir('./npy')\n",
    "np.save('./npy/x_train.npy', x_train)\n",
    "np.save('./npy/x_test.npy', x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idhi layer.py lo unna code\n",
    "\n",
    "def conv_layer(x, filter_shape, stride):\n",
    "    filters = tf.get_variable(\n",
    "        name='weight',\n",
    "        shape=filter_shape,\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.contrib.layers.xavier_initializer(),\n",
    "        trainable=True)\n",
    "    return tf.nn.conv2d(x, filters, [1, stride, stride, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def dilated_conv_layer(x, filter_shape, dilation):\n",
    "    filters = tf.get_variable(\n",
    "        name='weight',\n",
    "        shape=filter_shape,\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.contrib.layers.xavier_initializer(),\n",
    "        trainable=True)\n",
    "    return tf.nn.atrous_conv2d(x, filters, dilation, padding='SAME')\n",
    "\n",
    "\n",
    "def deconv_layer(x, filter_shape, output_shape, stride):\n",
    "    filters = tf.get_variable(\n",
    "        name='weight',\n",
    "        shape=filter_shape,\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.contrib.layers.xavier_initializer(),\n",
    "        trainable=True)\n",
    "    return tf.nn.conv2d_transpose(x, filters, output_shape, [1, stride, stride, 1])\n",
    "\n",
    "\n",
    "def batch_normalize(x, is_training, decay=0.99, epsilon=0.001):\n",
    "    def bn_train():\n",
    "        batch_mean, batch_var = tf.nn.moments(x, axes=[0, 1, 2])\n",
    "        train_mean = tf.assign(pop_mean, pop_mean * decay + batch_mean * (1 - decay))\n",
    "        train_var = tf.assign(pop_var, pop_var * decay + batch_var * (1 - decay))\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(x, batch_mean, batch_var, beta, scale, epsilon)\n",
    "\n",
    "    def bn_inference():\n",
    "        return tf.nn.batch_normalization(x, pop_mean, pop_var, beta, scale, epsilon)\n",
    "\n",
    "    dim = x.get_shape().as_list()[-1]\n",
    "    beta = tf.get_variable(\n",
    "        name='beta',\n",
    "        shape=[dim],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.truncated_normal_initializer(stddev=0.0),\n",
    "        trainable=True)\n",
    "    scale = tf.get_variable(\n",
    "        name='scale',\n",
    "        shape=[dim],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.truncated_normal_initializer(stddev=0.1),\n",
    "        trainable=True)\n",
    "    pop_mean = tf.get_variable(\n",
    "        name='pop_mean',\n",
    "        shape=[dim],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.constant_initializer(0.0),\n",
    "        trainable=False)\n",
    "    pop_var = tf.get_variable(\n",
    "        name='pop_var',\n",
    "        shape=[dim],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.constant_initializer(1.0),\n",
    "        trainable=False)\n",
    "\n",
    "    return tf.cond(is_training, bn_train, bn_inference)\n",
    "\n",
    "\n",
    "def flatten_layer(x):\n",
    "    input_shape = x.get_shape().as_list()\n",
    "    dim = input_shape[1] * input_shape[2] * input_shape[3]\n",
    "    transposed = tf.transpose(x, (0, 3, 1, 2))\n",
    "    return tf.reshape(transposed, [-1, dim])\n",
    "\n",
    "\n",
    "def full_connection_layer(x, out_dim):\n",
    "    in_dim = x.get_shape().as_list()[-1]\n",
    "    W = tf.get_variable(\n",
    "        name='weight',\n",
    "        shape=[in_dim, out_dim],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.truncated_normal_initializer(stddev=0.1),\n",
    "        trainable=True)\n",
    "    b = tf.get_variable(\n",
    "        name='bias',\n",
    "        shape=[out_dim],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.constant_initializer(0.0),\n",
    "        trainable=True)\n",
    "    return tf.add(tf.matmul(x, W), b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idhi load.py lo code\n",
    "\n",
    "def load(dir_='../data/npy'):\n",
    "    x_train = np.load(os.path.join(dir_, 'x_train.npy'))\n",
    "    x_test = np.load(os.path.join(dir_, 'x_test.npy'))\n",
    "    return x_train, x_test\n",
    "\n",
    "'''\n",
    "if __name__ == '__main__':\n",
    "    x_train, x_test = load()\n",
    "    print(x_train.shape)\n",
    "    print(x_test.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# network.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idhi network.py lo unna code\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, x, mask, local_x, global_completion, local_completion, is_training, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.imitation = self.generator(x * (1 - mask), is_training)\n",
    "        self.completion = self.imitation * mask + x * (1 - mask)\n",
    "        self.real = self.discriminator(x, local_x, reuse=False)\n",
    "        self.fake = self.discriminator(global_completion, local_completion, reuse=True)\n",
    "        self.g_loss = self.calc_g_loss(x, self.completion)\n",
    "        self.d_loss = self.calc_d_loss(self.real, self.fake)\n",
    "        self.g_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator')\n",
    "        self.d_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='discriminator')\n",
    "\n",
    "\n",
    "    def generator(self, x, is_training):\n",
    "        with tf.variable_scope('generator'):\n",
    "            with tf.variable_scope('conv1'):\n",
    "                x = conv_layer(x, [5, 5, 3, 64], 1)\n",
    "                x = batch_normalize(x, is_training)\n",
    "                x = tf.nn.relu(x)\n",
    "            with tf.variable_scope('conv2'):\n",
    "                x = conv_layer(x, [3, 3, 64, 128], 2)\n",
    "                x = batch_normalize(x, is_training)\n",
    "                x = tf.nn.relu(x)\n",
    "            with tf.variable_scope('conv3'):\n",
    "                x = conv_layer(x, [3, 3, 128, 128], 1)\n",
    "                x = batch_normalize(x, is_training)\n",
    "                x = tf.nn.relu(x)\n",
    "            with tf.variable_scope('conv4'):\n",
    "                x = conv_layer(x, [3, 3, 128, 256], 2)\n",
    "                x = batch_normalize(x, is_training)\n",
    "                x = tf.nn.relu(x)\n",
    "            with tf.variable_scope('conv5'):\n",
    "                x = conv_layer(x, [3, 3, 256, 256], 1)\n",
    "                x = batch_normalize(x, is_training)\n",
    "                x = tf.nn.relu(x)\n",
    "            with tf.variable_scope('conv6'):\n",
    "                x = conv_layer(x, [3, 3, 256, 256], 1)\n",
    "                x = batch_normalize(x, is_training)\n",
    "                x = tf.nn.relu(x)\n",
    "            with tf.variable_scope('dilated1'):\n",
    "                x = dilated_conv_layer(x, [3, 3, 256, 256], 2)\n",
    "                x = batch_normalize(x, is_training)\n",
    "                x = tf.nn.relu(x)\n",
    "            with tf.variable_scope('dilated2'):\n",
    "                x = dilated_conv_layer(x, [3, 3, 256, 256], 4)\n",
    "                x = batch_normalize(x, is_training)\n",
    "                x = tf.nn.relu(x)\n",
    "            with tf.variable_scope('dilated3'):\n",
    "                x = dilated_conv_layer(x, [3, 3, 256, 256], 8)\n",
    "                x = batch_normalize(x, is_training)\n",
    "                x = tf.nn.relu(x)\n",
    "            with tf.variable_scope('dilated4'):\n",
    "                x = dilated_conv_layer(x, [3, 3, 256, 256], 16)\n",
    "                x = batch_normalize(x, is_training)\n",
    "                x = tf.nn.relu(x)\n",
    "            with tf.variable_scope('conv7'):\n",
    "                x = conv_layer(x, [3, 3, 256, 256], 1)\n",
    "                x = batch_normalize(x, is_training)\n",
    "                x = tf.nn.relu(x)\n",
    "            with tf.variable_scope('conv8'):\n",
    "                x = conv_layer(x, [3, 3, 256, 256], 1)\n",
    "                x = batch_normalize(x, is_training)\n",
    "                x = tf.nn.relu(x)\n",
    "            with tf.variable_scope('deconv1'):\n",
    "                x = deconv_layer(x, [4, 4, 128, 256], [self.batch_size, 64, 64, 128], 2)\n",
    "                x = batch_normalize(x, is_training)\n",
    "                x = tf.nn.relu(x)\n",
    "            with tf.variable_scope('conv9'):\n",
    "                x = conv_layer(x, [3, 3, 128, 128], 1)\n",
    "                x = batch_normalize(x, is_training)\n",
    "                x = tf.nn.relu(x)\n",
    "            with tf.variable_scope('deconv2'):\n",
    "                x = deconv_layer(x, [4, 4, 64, 128], [self.batch_size, 128, 128, 64], 2)\n",
    "                x = batch_normalize(x, is_training)\n",
    "                x = tf.nn.relu(x)\n",
    "            with tf.variable_scope('conv10'):\n",
    "                x = conv_layer(x, [3, 3, 64, 32], 1)\n",
    "                x = batch_normalize(x, is_training)\n",
    "                x = tf.nn.relu(x)\n",
    "            with tf.variable_scope('conv11'):\n",
    "                x = conv_layer(x, [3, 3, 32, 3], 1)\n",
    "                x = tf.nn.tanh(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def discriminator(self, global_x, local_x, reuse):\n",
    "        def global_discriminator(x):\n",
    "            is_training = tf.constant(True)\n",
    "            with tf.variable_scope('global'):\n",
    "                with tf.variable_scope('conv1'):\n",
    "                    x = conv_layer(x, [5, 5, 3, 64], 2)\n",
    "                    x = batch_normalize(x, is_training)\n",
    "                    x = tf.nn.relu(x)\n",
    "                with tf.variable_scope('conv2'):\n",
    "                    x = conv_layer(x, [5, 5, 64, 128], 2)\n",
    "                    x = batch_normalize(x, is_training)\n",
    "                    x = tf.nn.relu(x)\n",
    "                with tf.variable_scope('conv3'):\n",
    "                    x = conv_layer(x, [5, 5, 128, 256], 2)\n",
    "                    x = batch_normalize(x, is_training)\n",
    "                    x = tf.nn.relu(x)\n",
    "                with tf.variable_scope('conv4'):\n",
    "                    x = conv_layer(x, [5, 5, 256, 512], 2)\n",
    "                    x = batch_normalize(x, is_training)\n",
    "                    x = tf.nn.relu(x)\n",
    "                with tf.variable_scope('conv5'):\n",
    "                    x = conv_layer(x, [5, 5, 512, 512], 2)\n",
    "                    x = batch_normalize(x, is_training)\n",
    "                    x = tf.nn.relu(x)\n",
    "                with tf.variable_scope('fc'):\n",
    "                    x = flatten_layer(x)\n",
    "                    x = full_connection_layer(x, 1024)\n",
    "            return x\n",
    "\n",
    "        def local_discriminator(x):\n",
    "            is_training = tf.constant(True)\n",
    "            with tf.variable_scope('local'):\n",
    "                with tf.variable_scope('conv1'):\n",
    "                    x = conv_layer(x, [5, 5, 3, 64], 2)\n",
    "                    x = batch_normalize(x, is_training)\n",
    "                    x = tf.nn.relu(x)\n",
    "                with tf.variable_scope('conv2'):\n",
    "                    x = conv_layer(x, [5, 5, 64, 128], 2)\n",
    "                    x = batch_normalize(x, is_training)\n",
    "                    x = tf.nn.relu(x)\n",
    "                with tf.variable_scope('conv3'):\n",
    "                    x = conv_layer(x, [5, 5, 128, 256], 2)\n",
    "                    x = batch_normalize(x, is_training)\n",
    "                    x = tf.nn.relu(x)\n",
    "                with tf.variable_scope('conv4'):\n",
    "                    x = conv_layer(x, [5, 5, 256, 512], 2)\n",
    "                    x = batch_normalize(x, is_training)\n",
    "                    x = tf.nn.relu(x)\n",
    "                with tf.variable_scope('fc'):\n",
    "                    x = flatten_layer(x)\n",
    "                    x = full_connection_layer(x, 1024)\n",
    "            return x\n",
    "\n",
    "        with tf.variable_scope('discriminator', reuse=reuse):\n",
    "            global_output = global_discriminator(global_x)\n",
    "            local_output = local_discriminator(local_x)\n",
    "            with tf.variable_scope('concatenation'):\n",
    "                output = tf.concat((global_output, local_output), 1)\n",
    "                output = full_connection_layer(output, 1)\n",
    "               \n",
    "        return output\n",
    "\n",
    "\n",
    "    def calc_g_loss(self, x, completion):\n",
    "        loss = tf.nn.l2_loss(x - completion)\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "    def calc_d_loss(self, real, fake):\n",
    "        alpha = 4e-4\n",
    "        d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real, labels=tf.ones_like(real)))\n",
    "        d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake, labels=tf.zeros_like(fake)))\n",
    "        return tf.add(d_loss_real, d_loss_fake) * alpha\n",
    "\n",
    "# network.py code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256 #image size ni 256 ki maarcha\n",
    "LOCAL_SIZE = 64\n",
    "HOLE_MIN = 24\n",
    "HOLE_MAX = 48\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 16\n",
    "PRETRAIN_EPOCH = 100\n",
    "\n",
    "def train():\n",
    "    x = tf.placeholder(tf.float32, [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 3])\n",
    "    mask = tf.placeholder(tf.float32, [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 1])\n",
    "    local_x = tf.placeholder(tf.float32, [BATCH_SIZE, LOCAL_SIZE, LOCAL_SIZE, 3])\n",
    "    global_completion = tf.placeholder(tf.float32, [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 3])\n",
    "    local_completion = tf.placeholder(tf.float32, [BATCH_SIZE, LOCAL_SIZE, LOCAL_SIZE, 3])\n",
    "    is_training = tf.placeholder(tf.bool, [])\n",
    "\n",
    "    model = Network(x, mask, local_x, global_completion, local_completion, is_training, batch_size=BATCH_SIZE)\n",
    "# Session is used for performing some operations to tensors\n",
    "    sess = tf.Session()\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    epoch = tf.Variable(0, name='epoch', trainable=False)\n",
    "\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "    g_train_op = opt.minimize(model.g_loss, global_step=global_step, var_list=model.g_variables)\n",
    "    d_train_op = opt.minimize(model.d_loss, global_step=global_step, var_list=model.d_variables)\n",
    "\n",
    "# initializes all variables\n",
    "    init_op = tf.global_variables_initializer()\n",
    "# Runs init_op which initializes all the created variables which are created using tf.Variable\n", 
    "    sess.run(init_op)\n",
    "\n",
    "    if tf.train.get_checkpoint_state('./backup'):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, './backup/latest')\n",
    "\n",
    "    x_train, x_test = load.load()\n",
    "# why this?? IDK..\n",
    "    x_train = np.array([a / 127.5 - 1 for a in x_train])\n",
    "    x_test = np.array([a / 127.5 - 1 for a in x_test])\n",
    "\n",
    "# similar to steps per epoch\n",
    "    step_num = int(len(x_train) / BATCH_SIZE)\n",
    "\n",
    "    while True:\n",
    "# Running the model for every epoch\n",
    "        sess.run(tf.assign(epoch, tf.add(epoch, 1)))\n",
    "        print('epoch: {}'.format(sess.run(epoch)))\n",
    "\n",
    "        np.random.shuffle(x_train)\n",
    "\n",
    "        # Completion \n",
    "        if sess.run(epoch) <= PRETRAIN_EPOCH:\n",
    "            g_loss_value = 0\n",
    "            for i in tqdm.tqdm(range(step_num)):\n",
    "                # for every progress a sample train array is taken and mask is calculated\n",
    "# x_batch is [0:16],[16:32],...so on\n",
    "                x_batch = x_train[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]\n",
    "                points_batch, mask_batch = get_points()\n",
    "\n",
    "               # g_train_op is optimizer to minimize generator\n",
    "               # the masked values are being fed here which for every iteration calculates masks and sends to the network\n",
    "                _, g_loss = sess.run([g_train_op, model.g_loss], feed_dict={x: x_batch, mask: mask_batch, is_training: True})\n",
    "                g_loss_value += g_loss\n",
    "\n",
    "            print('Completion loss: {}'.format(g_loss_value))\n",
    "\n",
    "            np.random.shuffle(x_test) \n",
    "            x_batch = x_test[:BATCH_SIZE]\n",
    "# completion is used for generating a complete image from a mask and image\n",
    "            completion = sess.run(model.completion, feed_dict={x: x_batch, mask: mask_batch, is_training: False})\n",
    "            sample = np.array((completion[0] + 1) * 127.5, dtype=np.uint8)\n",
    "            cv2.imwrite('./output/{}.jpg'.format(\"{0:06d}\".format(sess.run(epoch))), cv2.cvtColor(sample, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(sess, './backup/latest', write_meta_graph=False)\n",
    "            if sess.run(epoch) == PRETRAIN_EPOCH:\n",
    "                saver.save(sess, './backup/pretrained', write_meta_graph=False)\n",
    "\n",
    "\n",
    "        # Discrimitation\n",
    "        else:\n",
    "            g_loss_value = 0\n",
    "            d_loss_value = 0\n",
    "            for i in tqdm.tqdm(range(step_num)):\n",
    "                x_batch = x_train[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]\n",
    "                points_batch, mask_batch = get_points()\n",
    "\n",
    "                _, g_loss, completion = sess.run([g_train_op, model.g_loss, model.completion], feed_dict={x: x_batch, mask: mask_batch, is_training: True})\n",
    "                g_loss_value += g_loss\n",
    "\n",
    "                local_x_batch = []\n",
    "                local_completion_batch = []\n",
    "                for i in range(BATCH_SIZE):\n",
    "                    x1, y1, x2, y2 = points_batch[i]\n",
    "                    local_x_batch.append(x_batch[i][y1:y2, x1:x2, :])\n",
    "                    local_completion_batch.append(completion[i][y1:y2, x1:x2, :])\n",
    "                local_x_batch = np.array(local_x_batch)\n",
    "                local_completion_batch = np.array(local_completion_batch)\n",
    "\n",
    "                _, d_loss = sess.run(\n",
    "                    [d_train_op, model.d_loss], \n",
    "                    feed_dict={x: x_batch, mask: mask_batch, local_x: local_x_batch, global_completion: completion, local_completion: local_completion_batch, is_training: True})\n",
    "                d_loss_value += d_loss\n",
    "\n",
    "            print('Completion loss: {}'.format(g_loss_value))\n",
    "            print('Discriminator loss: {}'.format(d_loss_value))\n",
    "\n",
    "            np.random.shuffle(x_test) \n",
    "            x_batch = x_test[:BATCH_SIZE]\n",
    "            completion = sess.run(model.completion, feed_dict={x: x_batch, mask: mask_batch, is_training: False})\n",
    "            sample = np.array((completion[0] + 1) * 127.5, dtype=np.uint8)\n",
    "            cv2.imwrite('./output/{}.jpg'.format(\"{0:06d}\".format(sess.run(epoch))), cv2.cvtColor(sample, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(sess, './backup/latest', write_meta_graph=False)\n",
    "\n",
    "\n",
    "def get_points():\n",
    "    points = []\n",
    "    mask = []\n",
    "    for i in range(BATCH_SIZE):\n",
    "        x1, y1 = np.random.randint(0, IMAGE_SIZE - LOCAL_SIZE + 1, 2)\n",
    "        x2, y2 = np.array([x1, y1]) + LOCAL_SIZE\n",
    "        points.append([x1, y1, x2, y2])\n",
    "\n",
    "        w, h = np.random.randint(HOLE_MIN, HOLE_MAX + 1, 2)\n",
    "        p1 = x1 + np.random.randint(0, LOCAL_SIZE - w)\n",
    "        q1 = y1 + np.random.randint(0, LOCAL_SIZE - h)\n",
    "        p2 = p1 + w\n",
    "        q2 = q1 + h\n",
    "        \n",
    "        m = np.zeros((IMAGE_SIZE, IMAGE_SIZE, 1), dtype=np.uint8)\n",
    "        m[q1:q2 + 1, p1:p2 + 1] = 1\n",
    "        mask.append(m)\n",
    "\n",
    "\n",
    "    return np.array(points), np.array(mask)\n",
    "\n",
    "'''\n",
    "if __name__ == '__main__':\n",
    "    train()\n",
    "'''    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
